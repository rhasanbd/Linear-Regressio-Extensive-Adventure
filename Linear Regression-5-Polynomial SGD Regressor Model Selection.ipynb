{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression - Gradient Descent Model Selection\n",
    "\n",
    "In this notebook, we show how to perform model selection when using Stochastic Gradient Descent (SGD) algorithm for a Polynomial Regression Model. There are two sets of hyperparameters for SGD based Polynomial Regression model.\n",
    "\n",
    "- Polynomial degree\n",
    "- SGD algorithm hyperparameters (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "\n",
    "\n",
    "## Data Transformation and Model Creation\n",
    "\n",
    "The SGD Polynomial Regressor model requires the data to be augmented by the following two transformations.\n",
    "- Create polynomial features based on the degree of the polynomial\n",
    "- Standardize the augmented dataset\n",
    "\n",
    "Then, a SGD Regressor model is created, which uses the augmented data for training.\n",
    "\n",
    "For model selection, we need to create several models based on the polynomial degree as well as combinations of a range of values of the SGD Regressor hyperparameters.\n",
    "\n",
    "The above two data transformation steps can be combined in a single model step by using Scikit-Learn's Pipeline object.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "The Pipeline object will assemble several steps that can be cross-validated together while setting different parameters. More specifically it will perform the following three steps to create a SGD Polynomial Regression model.\n",
    "\n",
    "- Create polynomial features based on the degree of the polynomial\n",
    "- Standardize the augmented dataset\n",
    "- Create a SGD Regressor model, which will use the standardized data for training\n",
    "\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We use the Boston housing dataset that provides housing values in the suburbs of Boston.\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "\n",
    "The **MEDV** variable is the target variable.\n",
    "\n",
    "### Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "- CRIM: per capita crime rate by town.\n",
    "\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS: proportion of non-retail business acres per town.\n",
    "\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- RM: average number of rooms per dwelling.\n",
    "\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- DIS: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- RAD: index of accessibility to radial highways.\n",
    "\n",
    "- TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- LSTAT: lower status of the population (percent).\n",
    "\n",
    "- MEDV: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "print(boston.data.shape, boston.target.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df = pd.concat([df, pd.Series(boston.target,name='MEDV')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Separate Feature Set (Data Matrix X) and Target (1D Array y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target array (y) containing the target.\n",
    "\n",
    "First, we create separate data frame objects for X and y. Then, we convert the data frame objects into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Matrix (X) Shape:  (506, 13)\n",
      "Label Array (y) Shape:  (506,)\n",
      "\n",
      "Data Matrix (X) Type:  float64\n",
      "Label Array (y) Type:  float64\n"
     ]
    }
   ],
   "source": [
    "# Create separate data frame objects for X (features) and y (target)\n",
    "X = df.drop(columns='MEDV')  \n",
    "y = df['MEDV'] \n",
    "\n",
    "\n",
    "X = np.asarray(X) # Data Matrix containing all features excluding the target\n",
    "y = np.asarray(y) # 1D target array\n",
    "\n",
    "\n",
    "print(\"Data Matrix (X) Shape: \", X.shape)\n",
    "print(\"Label Array (y) Shape: \", y.shape)\n",
    "\n",
    "print(\"\\nData Matrix (X) Type: \", X.dtype)\n",
    "print(\"Label Array (y) Type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "For hyperparameter tuning, we build a compund regressor using the Scikit-Learn’s  Pipeline class. It will combine the PolynomialFeatures(), StandardScaler() and SGDRegressor() objects and will create a single object.\n",
    "\n",
    "The best way to do hyperparameter tuning is to use cross-validation. We will use Scikit-Learn’s GridSearchCV to search the combinations of hyperparameter values that provide best performance.\n",
    "\n",
    "We need to tell which hyperparameters we want the GridSearchCV to experiment with, and what values to try out. It will evaluate all the possible combinations of hyperparameter values, using cross-validation.\n",
    "\n",
    "To denote the module objects in the Pipeline we use arbitrary names: \"poly\", \"scaler\" and \"sgd\". We use these names to perform grid search for the respective optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1068 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1341 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1691 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -15.580378\n",
      "Optimal Hyperparameter Values:  {'poly__degree': 2, 'sgd__alpha': 0.001, 'sgd__eta0': 0.01, 'sgd__l1_ratio': 0.5, 'sgd__max_iter': 1000}\n",
      "\n",
      "\n",
      "CPU times: user 4.87 s, sys: 407 ms, total: 5.27 s\n",
      "Wall time: 5min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed:  5.3min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a Pipeline object\n",
    "sgd_pipeline = Pipeline([\n",
    "        # Bias should be excluded because by default SGDRegressor adds bias via the\"fit_intercept\" parameter\n",
    "        ('poly', PolynomialFeatures(include_bias=False)), \n",
    "        ('scaler', StandardScaler()),\n",
    "        ('sgd', SGDRegressor(penalty='elasticnet')),\n",
    "    ])\n",
    "\n",
    "# Create a dictionary object with hyperparameters as keys and lists of corresponding values\n",
    "param_grid = {'poly__degree': [1, 2, 3, 4, 5],\n",
    "              'sgd__alpha': [0.1, 0.01, 0.001, 0.0001], \n",
    "              'sgd__l1_ratio': [1, 0.7, 0.5, 0.2, 0], 'sgd__max_iter':[500, 1000],\n",
    "              'sgd__eta0': [0.01, 0.001, 0.0001]}\n",
    "\n",
    "# Create a GridSearchCV object and perform hyperparameter tuning\n",
    "sgd = GridSearchCV(sgd_pipeline, param_grid, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# The model is trained with optimal hyperparameters, thus its the optimal model\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "params_optimal_sgd = sgd.best_params_\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % sgd.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_sgd)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We use the **optimal model created above** for its evaluation based on training and test data. There is no need to train the model again by using the optimal hyperparameters.\n",
    "\n",
    "Note that the model is a Pipeline object. Before making predictions, it will transform the data by applying the optimal degree and standardization. Thus, we don't need to separately add polynomial terms and perform standardization for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Mean squared error: 11.40\n",
      "Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.87\n",
      "\n",
      "Test: Mean squared error: 13.19\n",
      "Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Training data: Make prediction \n",
    "y_train_predicted_sgd = sgd.predict(X_train)\n",
    "\n",
    "print(\"Train: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "# Training data: Explained variance score: 1 is perfect prediction\n",
    "print(\"Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % r2_score(y_train, y_train_predicted_sgd))\n",
    "\n",
    "# Test data: Make prediction \n",
    "y_test_predicted = sgd.predict(X_test)\n",
    "\n",
    "print(\"\\nTest: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "# Training data: Explained variance score: 1 is perfect prediction\n",
    "print(\"Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % r2_score(y_test, y_test_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
